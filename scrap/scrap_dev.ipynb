{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install requests html5lib bs4 re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Workflow\n",
    "import requests \n",
    "from bs4 import BeautifulSoup \n",
    "import re, json\n",
    "\n",
    "## A) Scrape https://onepiece.fandom.com/wiki/List_of_Canon_Characters for list of links towards all canon characters\n",
    "\n",
    "## B) Get rid of the fluff and save the html output for caching purposes (no overloading)\n",
    "\n",
    "## C) Scrape page as json based on Content\n",
    "### 1 Appearance\n",
    "### 2 Personality\n",
    "### 3 Abilities and Powers | Abilities | Power\n",
    "### 4 History\n",
    "### 5 References\n",
    "\n",
    "## C) Scrape for the Statistics and Devil Fruit informations (pi-items)\n",
    "\n",
    "#Test run with Alpaca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_name = \"Alpacaman\"\n",
    "character_url = f\"https://onepiece.fandom.com/wiki/{character_name}\"\n",
    "\n",
    "r = requests.get(character_url) \n",
    "soup = BeautifulSoup(r.content, 'html5lib')\n",
    "\n",
    "print(soup.div)\n",
    "print(character_url)\n",
    "# Add some logging / wait time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toc_pattern = re.compile(r'toclevel-1.*')\n",
    "tocs = soup.find_all(\"li\", class_=toc_pattern)\n",
    "\n",
    "def toc_to_json(tocs):\n",
    "    \"\"\"\n",
    "    Scrap table of content and returns a json.\n",
    "\n",
    "    Args:\n",
    "        tocs (str): html soup of table of contents, filtered through class patterns\n",
    "\n",
    "    Returns:\n",
    "        json\n",
    "        \n",
    "    Remarks:\n",
    "        #find_all recursive=False to only consider direct children and avoid dup entries with nested children\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    toc_data = []\n",
    "    for li in tocs:\n",
    "        entry = {\n",
    "            'name': li.find('span', class_='toctext').get_text(),\n",
    "            'children': toc_to_json(li.find('ul').find_all('li', recursive=False) if li.find('ul') else []) #recursive bs\n",
    "        }\n",
    "        toc_data.append(entry)\n",
    "    return toc_data\n",
    "\n",
    "toc_data = toc_to_json(tocs)\n",
    "toc_data\n",
    "#tocs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_with_spacing(element):\n",
    "    text = \"\"\n",
    "    last_text = \"\"\n",
    "    \n",
    "    for child in element.descendants:\n",
    "        if child.name == 'a':\n",
    "            # Extract text from hyperlink with surrounding spaces\n",
    "            hyperlink_text = ' ' + child.get_text(strip=True) + ' '\n",
    "            # Add hyperlink text if it's not a duplicate of the last added text\n",
    "            if hyperlink_text.strip() != last_text.strip():\n",
    "                text += hyperlink_text\n",
    "                last_text = hyperlink_text\n",
    "        elif child.string:\n",
    "            # Extract text from non-hyperlink elements\n",
    "            child_text = child.string\n",
    "            # Add child text if it's not a duplicate of the last added text\n",
    "            if child_text.strip() != last_text.strip():\n",
    "                text += child_text\n",
    "                last_text = child_text\n",
    "\n",
    "    # Clean up extra spaces around punctuation and possessives\n",
    "    text = re.sub(r'\\s+([.,!?\\'\":;])', r'\\1', text)  # Remove space before punctuation\n",
    "    text = re.sub(r'([\\'\":;])\\s+', r'\\1 ', text)  # Ensure space after punctuation\n",
    "    text = re.sub(r'(\\w)\\'s', r'\\1\\'s', text)  # Remove space before 's for possessives\n",
    "    text = re.sub(r'\\(\\s+', '(', text)  # Remove space after opening parenthesis\n",
    "    text = re.sub(r'\\s+\\)', ')', text)  # Remove space before closing parenthesis\n",
    "    text = re.sub(r'\\[\\s+', '[', text)  # Remove space after opening square bracket\n",
    "    text = re.sub(r'\\s+\\]', ']', text)  # Remove space before closing square bracket\n",
    "    \n",
    "    # Fix the issue with backslashes before apostrophes\n",
    "    text = text.replace(\"\\\\'\", \"'\")\n",
    "    \n",
    "    # Clean up extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "content_test = \"He calls this styleAlpaca Kenpo(アルパカ剣法,Arupaka Kenpō?, Viz/Funi: Alpaca Fencing).[4]\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_toc(toc_data, soup):\n",
    "    content_data = []\n",
    "    for item in toc_data: \n",
    "        \n",
    "        #TODO: Skip Site navigation or clean after ? Implemented for now, but need to decide. Maybe better to skip if need to add tables in the future (site nav data is in tables)\n",
    "        if item['name'].lower() == 'site navigation':\n",
    "            continue\n",
    "        \n",
    "        content_id = item['name'].replace(\" \", \"_\")\n",
    "        heading = soup.find(id = content_id).parent\n",
    "        content_text = \"\"\n",
    "        \n",
    "        def process_sibling(sibling):\n",
    "            nonlocal content_text\n",
    "            # Next section is a paragraph\n",
    "            if sibling.name == 'p':\n",
    "                content_text += extract_text_with_spacing(sibling) + \" \"\n",
    "            # Next section is an unordered or ordered list\n",
    "            elif sibling.name in ['ul', 'ol']:\n",
    "                for li in sibling.find_all('li'):\n",
    "                    content_text += extract_text_with_spacing(li) + \" \"\n",
    "            # Edge case if next section is a div, access it then run the above\n",
    "            elif sibling.name == 'div':\n",
    "                for sub_sibling in sibling.children:\n",
    "                    process_sibling(sub_sibling)\n",
    "        \n",
    "        for sibling in heading.find_next_siblings():\n",
    "            # Next section is a new heading, i.e. current is finished or empty\n",
    "            if sibling.name in ['h2', 'h3', 'h4', 'h5', 'h6']:\n",
    "                break\n",
    "            process_sibling(sibling)\n",
    "            \n",
    "        content_entry = {\n",
    "            'name': item['name'],\n",
    "            'content': content_text.strip(),\n",
    "            'children': enrich_toc((item['children'] if item['children'] else []), soup) #recursive bs\n",
    "        }\n",
    "        \n",
    "        content_data.append(content_entry)\n",
    "            \n",
    "    return content_data\n",
    "\n",
    "content_data = enrich_toc(toc_data, soup)\n",
    "print(json.dumps(content_data, indent=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Work In Progress\n",
    "\n",
    "category_mappings = {\n",
    "    \"appearance\": [\"Appearance\", \"Appearances\", \"Looks\"],\n",
    "    \"personality\": [\"Personality\", \"Character\"],\n",
    "    \"abilities_and_powers\": [\"Abilities and Powers\", \"Abilities\", \"Powers\"],\n",
    "    \"devil_fruit\": [\"Devil Fruit\"],\n",
    "    \"anime_only_techniques\": [\"Anime-Only Techniques\"],\n",
    "    \"weapons\": [\"Weapons\"],\n",
    "    \"haki\": [\"Haki\"],\n",
    "    \"history\": [\"History\"],\n",
    "    \"wano_country_arc\": [\"Wano Country Arc\"],\n",
    "    \"major_battles\": [\"Major Battles\"],\n",
    "    \"references\": [\"References\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_key(key, mappings):\n",
    "    for standard, synonyms in mappings.items():\n",
    "        if key in synonyms:\n",
    "            return standard\n",
    "    return key.lower().replace(\" \", \"_\")  # default conversion for unmapped categories\n",
    "\n",
    "def process_item(item, mappings):\n",
    "    category_name = normalize_key(item['name'], mappings)\n",
    "    content = item.get(\"content\", \"\").strip()\n",
    "    subcategories = item.get(\"children\", [])\n",
    "    \n",
    "    processed = [content]\n",
    "    \n",
    "    for subcat in subcategories:\n",
    "        subcat_name = normalize_key(subcat['name'], mappings)\n",
    "        subcat_content = process_item(subcat, mappings)\n",
    "        \n",
    "        if len(subcat_content) == 1:\n",
    "            processed.append({subcat_name: subcat_content})\n",
    "        else:\n",
    "            if isinstance(processed[-1], dict) and subcat_name in processed[-1]:\n",
    "                processed[-1][subcat_name].extend(subcat_content)\n",
    "            else:\n",
    "                processed.append({subcat_name: subcat_content})\n",
    "    \n",
    "    return processed\n",
    "\n",
    "def transform_json(character_name, data, mappings):\n",
    "    normalized_data = {\n",
    "        \"name\": f\"{character_name}\",\n",
    "        \"overview\": {},  # Add overview data when I come around to doing it, fckng done with scraping\n",
    "        \"details\": {}\n",
    "    }\n",
    "    \n",
    "    details = {}\n",
    "    for item in data:\n",
    "        category_content = process_item(item, mappings)\n",
    "        category_name = normalize_key(item['name'], mappings)\n",
    "        if category_name in details:\n",
    "            details[category_name].extend(category_content)\n",
    "        else:\n",
    "            details[category_name] = category_content\n",
    "\n",
    "    normalized_data[\"details\"] = details\n",
    "    \n",
    "    return normalized_data\n",
    "\n",
    "\n",
    "normalized_json = transform_json(character_name, content_data, category_mappings)\n",
    "print(json.dumps(normalized_json, indent=3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pypr",
   "language": "python",
   "name": "pypr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
